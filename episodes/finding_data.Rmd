---
title: "Finding multi-dimensional biodiversity data"
teaching: 10
exercises: 2
editor_options: 
  markdown: 
    wrap: 50
---

::: questions

- Where do multi-dimensional biodiversity data live online?
- How can we use R APIs to quickly and reproducibly retrieve data?
- How can we combine online resources to assemble and store multi-dimensional datasets?

:::

::::::::::::::::::::::::::::::::::::: objectives

After following this episode, we intend that participants will be able to:

1. Effectively search for and find trait, sequence, and abundance data online given a taxon and research question 
2. Retrieve data from GEOME & GBIF using their R APIs and apply API principles to other databases
3. Organize a dataset of multiple dimensions of biodiversity from multiple online sources

::::::::::::::::::::::::::::::::::::::::::::::::

# Setup

Later on in this episode, we'll be working with some R packages that wrap the APIs of online repositories. 
Let's set those up now. 

```{r load packages}

library(rgbif)
library(devtools)
devtools::install_github("biocodellc/fimsR-access")
library(geomedb)

```

# Sources of multidimensional biodiversity data: large open-access databases

In recent years, large grant-supported open-access databases have been growing at an incredible pace. 
Databases like this host data from a wide variety of sources. Often data is accumulated from other large projects and organizations.
These platforms are then used to share and standardize data for the widest possible use. 

Many of these databases exist, but here are some examples:
  GBIF contains occurrance records of individuals assembled from dozens of smaller databases.
  NCBI (National Center for Biotechnology Information) database, which includes GenBank is the largest repository of genetic data in the world, but includes a lot of biomedical data
  GEOME (Genomic Observatories MetaDatabase) contains genetic data associated with specimens or samples from the field. 
  OTOL (Open Tree of Life) is a database that combines published trees and taxonomies into one supertree 
  BOLD (Barcode of Life) 
  EOL (Encyclopedia of Life)
  
## Using R APIs to download database data 

An API is a set of tools that allows users to interact with a database, such as reading, writing, updating, and deleting data.
We can't go over all of these databases in detail, but let's cover some examples that illustrate some principles of how these APIs work for different data types. 

Lets download some occurrance data from GBIF, some sequence data from NBCI, and a phylogeny from OTOL.

Perhaps the simplest search you can do is for records of one or more species in a given database.

The spocc package is all about occurrence data and lets you download occurrences from all major occurrence databases. 
For GBIF access using the spocc package, we use the occ() function.

We'll put a species name as the 'query' and specify gbif as our target database in the 'from' field.

```{r sp_query}

df <- occ(query = 'Accipiter striatus', from = 'gbif')

```

For NCBI access using the rentez package we can also search for a species, but unlike spocc this returns a set of record IDs rather than the actual records.

```{r sp_query2}

search_results <- entrez_search(db="sra",
              term="Tetrahymena thermophila[ORGN]",
              retmax=0)
```

We then have to use entrez_summary() to obtain the actual record information.

```{r sp_query3}

summaries <- entrez_summary(db="sra",id=search_results$ids)

```

OTOL works similarly in that we need two steps, but the first search returns a set of resolved taxon IDs organized in a dataframe, not record IDs like NCBI.

```{r sp_query4}

resolved_names <- tnrs_match_names(c("Pongo", "Pan", "Gorilla", "Hoolock", "Homo")))

```

We can then get the tree containing only these matched taxa as tips

```{r sp_query5}

tr <- tol_induced_subtree(ott_ids = ott_id(resolved_names))

```

In addition to or instead of querying by species, we can search for records in a given location in the world.
Within spocc we need to pass location queries directly to GBIF using gbifopts

```{r loc_query1}

df <- occ(query = 'Accipiter striatus', from = 'gbif', has_coords=TRUE, gbifopts=list("continent"="north_america"))
df <- occ(query = 'Accipiter striatus', from = 'gbif', has_coords=TRUE, gbifopts=list("decimalLatitude"='30,35',"decimalLongitude='-30,-25'))

```

For NCBI, we can use the "country" argument 

```{r loc_query2}

search_results <- entrez_search(db="sra",
              term="Tetrahymena thermophila[ORGN]", country="US", 
              retmax=0)

```

Now let's take a closer look at the data we have downloaded and some similarities in how MDBD database data is formatted.

For GBIF, 

```{r format_1}

df

```


## Common issues with API big data downloads 

Download limits

Query limits 

# Sources of MDBD: "small data" attached to papers 

A lot of useful data is not held in big databases, and is instead attached to papers. 
Searching on Google and Google Scholar can be an effective way to find such data. 
Let's search for "mammal trait database" on Google Scholar.

The first paper looks good, so let's look at the Supplementary Data which will contain their data.

Let's download and unzip the data. 

Reading the metadata file, we see that a certain data file contains the traits. Great! Let's read it into R. 

# Assembling a combined dataset


# Recap

::: keypoints

- Many sources exist online where multidimensional biodiversity data can be obtained
- APIs allow for fast and reproducible querying and downloading
- Storing multidimensional data efficiently is important

:::
    
